# More challenging example: Ebola in the Democratic Republic of Congo

Here we will explore a more challenging example, that of an outbreak of Ebola in the Democratic Republic of Congo in 1995. Details can be found in [@mckinleyetal:2009]. Firstly, load the `SimBIID` library:

```{r, sim-simbiid, warning = F, message = F}
## load library
library(SimBIID)
```

This in an $SEIR$ model, where the rates of transition between the states is given by:

\begin{align*}
    P\left[\mbox{infection event in}~[t, t + \delta t)\right] = \beta S I / N + o(\delta t)\\
    P\left[\mbox{infectivity event in}~[t, t + \delta t)\right] = \delta E + o(\delta t)\\
    P\left[\mbox{removal event in}~[t, t + \delta t)\right] = \gamma I + o(\delta t)
\end{align*}

where $\beta$ is *time-dependent*, such that:

$$
    \beta = \left\{
    \begin{align}
        &\beta & \mathrm{if}~t < t_{int},\\
        &\beta e^{-q(t - t_{int})} & \mathrm{if}~t \geq t_{int},
    \end{align}\right.
$$

where $t_{int}$ is the time at which intervention strategies were introduced (where $t_{int} = 123$ in this example). The challenge here is that in a stochastic system we require that the event rates are piecewise constant, and thus the continuous exponential decay is hard to capture. Instead we can approximate this by assuming that the rate only changes after each event (in practice we would do this in a much better manner at the cost of an increased computational burden).

To model this using our `mparseRcpp()` function, we need to add a little bit of additional C code to our model. In essence we need an **if-else** statement: **if** $t > t_{int}$, then multiply $\beta$ by $e^{-q(t - t_{int})}$, otherwise multiply it by 1. The C syntax for an if-else statement is of the form:

`(COND ? OUT1:OUT2)`

where `COND` is a condition, which if met outputs `OUT1`, else it outputs `OUT2`. Hence, we can use:

`(t > 123.0 ? (-q * (t - 123.0)):0.0)`

which returns a value of $-q(t - 123)$ if $t > 123.0$, or a value of $0$ if $t \leq 123$ (bearing in mind that $e^0 = 1$). Therefore, to set up the model we can run:

```{r, ebola-setmod}
## set up model
transitions <- c(
    "S -> beta * exp((t > 123.0 ? (-q * (t - 123.0)):0.0)) * S * I / (S + E + I + R) -> E", 
    "E -> delta * E -> I", 
    "I -> gamma * I -> R"
)
compartments <- c("S", "E", "I", "R")
pars <- c("beta", "delta", "gamma", "q")
model <- mparseRcpp(
    transitions = transitions, 
    compartments = compartments,
    pars = pars
)
```

```{r, ebola-compmod}
model <- compileRcpp(model)
model
```

```{task, title = "Question"}
Why is this an approximation? Is it an OK approximation do you think? Could you suggest a better approximation?
```

```{solution, title = "Answer"}
It's an approximation because we only update the rate every time there's an event. It might be an OK approximation, since if the rates are high, then we get lots of events and thus we update the infection rate more frequently. If rates are low, then we do not update the rates very frequently, but then the effect of the exponential decay is to set low rates after some period of time.

A better approach might be to update the infection rate at some regular interval, such as at daily time steps.
```

Now we can set priors and initial states (from McKinley et al., 2009):

```{r, ebola-setpriors}
## set priors
priors <- data.frame(
    parnames = c("beta", "delta", "gamma", "q"), 
    dist = rep("gamma", 4), 
    stringsAsFactors = F
)
priors$p1 <- rep(2, 4)
priors$p2 <- c(10, 10, 1 / 0.07, 10)

## set initial states
iniStates <- c(S = 5364499, E = 0, I = 1, R = 0)
```

Now we set up the data to match to (final epidemic size and date of final removal):

```{r, ebola-data}
## define the targeted summary statistics
data <- c(finalsize = 316, finaltime = 191)
```

Finally we set a sequence of tolerances:

```{r, ebola-tols}
## set tolerances
tols <- matrix(rep(round(seq(300, 40, length.out = 10)), each = 2), ncol = 2, byrow = T)
colnames(tols) <- c("finalsize", "finaltime")
```

We then set up a function to run the model, extract the final epidemic size and date of the final removal, and return the relevant measures:

```{r, ebola-func}
## function to match simulations
simEbola <- function(pars, data, tols, u, model) {
    ## run model
    sims <- model(pars, 0, data[2] + tols[2], u)
    
    ## this returns a vector of the form:
    ## completed (1/0), t, S, E, I, R (here)
    if(sims[1] == 0) {
        ## if simulation rejected
        return(NA)
    } else {
        ## extract finaltime and finalsize
        finaltime <- sims[2]
        finalsize <- sims[6]
    }
    
    ## return vector if match, else return NA
    if(all(abs(c(finalsize, finaltime) - data) <= tols)){
        return(c(finalsize, finaltime))
    } else {
        return(NA)
    }
}
```

```{r, ebola-seed}
## set seed
set.seed(50)
```

Now we run the ABC-SMC algorithm (initially just for a single generation and for 10 particles---you'll see why in a moment):

```{r, ebola-abcsmc}
## run ABC-SMC algorithm
post <- ABCSMC(data, priors, simEbola, iniStates, 10, tols[1, ], parallel = T, model = model)
```

```{task, title = "Question"}
We can see that the first generation of our algorithm took a long time to run. Why do you think this was?
```

```{solution, title = "Answer"}
This is for two main reasons. Firstly, the prior space is quite large relative to the posterior space, which means that a large number of parameters are generated that produce simulations that do not match the data. This means that lots of simulations have to be produce to get a fixed number of matches. Secondly, there is a large background population, which means that for some parameters we may simulate very large epidemics, which, although ultimately rejected, take a long time to evaluate.
```

## Speeding up simulations using stopping criteria

One way to speed up the algorithms is to note that if we have summary statistics that are **monotonically** increasing, then we can monitor these criteria during the simulation and reject the simulation as soon as the simulated summary measure is greater than the observed summary measure by more than the tolerance. This means that we do not have to run the simulation to completion to reject it.

We've already done this to a certain extent by only simulating up to time $T + \epsilon_T$, where $T$ is the final removal time. The rationale was the same; if the epidemic is still ongoing at time $T + \epsilon_T$, then by definition the difference between the simulated final removal time and the observed final removal time will be greater than the tolerance, and furthermore this distance will keep increasing. Hence we do not have to find out the exact final removal time in order to reject the simulation.

Here we will use a `stopCrit` argument to `mparseRcpp()` to define additional stopping criteria, based on the other summary statistics. The argument is defined as a **conditional statement** that defines when to reject. For example, to add a stopping criterion based on the final epidemic size, we can reject once $R^* > R_F + \epsilon_R$, where $R^*$ is the **simulated** number of removals and $R_F$ is the final epidemic size (the final **observed** number of removals). Hence,

```{r, ebola-setstop}
model <- mparseRcpp(
    transitions = transitions,
    compartments = compartments,
    pars = pars,
    addVars = c("finalsize", "tol_R"),
    stopCrit = "R > (finalsize + tol_R)"
)
```

Note that since the variables `finalsize` and `tol_R` do not exist in the model, we have to tell the parsing function to include them as additional arguments using the `addVars` argument. Once compiled, this adds to additional arguments to the `model()` function e.g.

```{r, ebola-compmod1}
model <- compileRcpp(model)
model
```

It is now up to us to make sure that we pass `finalsize` and `tol_R` to the `model()` function. We can do this by amending the `simEbola()` function we wrote earlier:

```{r, ebola-func1}
## function to match simulations
simEbola <- function(pars, data, tols, u, model) {
    ## run model
    sims <- model(pars, 0, data[2] + tols[2], u, data[1], tols[1])
    
    ## this returns a vector of the form:
    ## completed (1/0), t, S, E, I, R (here)
    if(sims[1] == 0) {
        ## if simulation rejected
        return(NA)
    } else {
        ## extract finaltime and finalsize
        finaltime <- sims[2]
        finalsize <- sims[6]
    }
    
    ## return vector if match, else return NA
    if(all(abs(c(finalsize, finaltime) - data) <= tols)){
        return(c(finalsize, finaltime))
    } else {
        return(NA)
    }
}
```

Now we run again:

```{r, ebola-abcsmc1}
## run ABC-SMC algorithm
post <- ABCSMC(data, priors, simEbola, iniStates, 10, tols[1, ], parallel = T, model = model)
```

Now we can see that the first generation in particular runs much faster than before since some simulations can be rejected sooner without loss of accuracy. Now we will increase the number of particles and run properly.

```{r, ebola-abcsmc2}
## run ABC-SMC algorithm
post <- ABCSMC(data, priors, simEbola, iniStates, 50, tols, parallel = T, model = model)
```

Since this ran relatively quickly, we'll now run for another few generations...

```{r, ebola-tols1}
tols <- matrix(rep(seq(30, 10, by = -10), each = 2), ncol = 2, byrow = T)
colnames(tols) <- c("finalsize", "finaltime")

## run ABC-SMC algorithm
post <- ABCSMC(post, tols, parallel = T)
```

```{task}
Produce posterior summary estimates for the derived variables of interest: $R_0$, the mean infectious period, and the mean incubation period. Plot the approximate posteriors and simulated outputs.
```

```{solution}

``{r, ebola-R0}
## function to calculate R0 and length of
## epidemiological periods
R0fn <- function(beta, delta, gamma) {
    data.frame(
        R0 = beta / gamma, 
        latperiod = 1 / delta, 
        infperiod = 1 / gamma
    )
}

## summarise approximate posterior
summary(post, transfunc = R0fn)
``

``{r, ebola-plotR0, fig.width = 8, fig.height = 8}
## plot approximate posteriors
plot(post, transfunc = R0fn, gen = c(1, 5, 13))
``

``{r, ebola-plotoutput}
## plot accepted outputs
plot(post, "output", gen = c(1, 5, 13))
``

```

## **Advanced task**: Working with more complex summary measures

The full data can be found in the "ebola.csv" file. The first column corresponds to the day from the introduction of the index case into the population. The second column corresponds to time series counts of the number of new cases exhibiting clinical signs at the end of each day. The final column contains time series counts of the number of removals at each time point.

We use the `clin_signs` column as a proxy for the $I$ **incidence** (not counts directly, rather **new** incidents at each day), and the `removals` column as a proxy for the $R$ **incidence**. Read the data into R as follows:
    
```{r, ebola-data}
ebola <- read.csv("ebola.csv", header = T)
head(ebola)
```

Notice that if we sum the events down each column we get different numbers of events. We know that the total epidemic size was 315, and hence we have `r 315 - sum(ebola$clin_signs)` missing $I$ events and `r 315 - sum(ebola$removals)` missing $R$ events.

```{task}
Amend the code above to add in a chi-squared distance metric (see below) between the simulated $I$ curve and the observed data curve (`clin_signs`), and also the simulated $R$ curve and the observed data curve (`removals`). **Note**: the simulations produce the **number of individuals** in each class at time $t$, whereas the data are the number of **new individuals** in each class at time $t$.

Another complexity is that monitoring of the epidemic did not start until day 55, and so we do not start generating the summary statistics until after that time point. Hence the chi-squared metric is defined as follows:
$$
    X^2 = \sum_{t = 55}^T \frac{(E_t - O_t)^2}{\max\left(O_t, 1\right)},
$$
where $O_t$ is the **observed** count at time $t$, and $E_t$ is the **simulated** count at time $t$ (notice that we don't start cumulating until day 55).

To calculate **removal incidence** we note that the number of removals at day $t$ is simply the number of removals at day $t - 1$ plus the number of new removals that occur in $(t - 1, t]$ (the **removal incidence**, denoted $i^R_t$) e.g.
$$
    R_t = R_{t - 1} + i^R_t,
$$ 
which implies that
$$
    i^R_t = R_t - R_{t - 1}.
$$
We can do a similar thing with the **infection incidence**, by noting that:
$$
    I_t = I_{t - 1} + i^I_t - i^R_t,
$$
and thus:
$$
    i^I_t = I_t - I_{t - 1} + i^R_t.
$$
We then substitute the values of $i^I_t$ for $E_t$ and the **observed** incidence for $O_t$ in the equation above to generate the metric. Similarly for the removal curve.

Run 10 generations of ABC-SMC, with tolerances uniformly distributed in the range 600--230 for the two chi-squared metrics, and 40--5 for the final epidemic size and time of the final removal. Summarise your approximate posteriors, and the posteriors for $R_0$, the mean latent period and the mean infectious period.
```

```{solution}

``{r, ebola-}
## generate model including tspan and stop criteria
model <- mparseRcpp(
    transitions = transitions,
    compartments = compartments,
    pars = pars,
    addVars = c("finalsize", "tol_R"),
    stopCrit = "R > (finalsize + tol_R)",
    tspan = T
)
## compile model
model <- compileRcpp(model)
model
``

Notice the order of the arguments in `model()`: pars`, `tstart`, `tstop`, `u`, `tspan`, `finalsize`, `tol_R`. Hence we need to amend our `simEbola()` function accordingly by adding a `tspan` argument, as well as vectors of observed counts:

``{r, ebola-solfunc}
## function to match simulations
simEbola <- function(pars, data, tols, u, model, tspan, OI, OR) {
    ## run model
    sims <- model(pars, 0, data[2] + tols[2], u, tspan, data[1], tols[1])
    
    ## 'sims' is now a list, with the first element a vector
    ## of the form: completed (1/0), t, S, E, I, R (here),
    ## and the second element is a matrix containing the 
    ## time-series counts
    simSum <- sims[[1]]
    counts <- sims[[2]]
    if(simSum[1] == 0) {
        ## if rejected during the simulation
        return(NA)
    } else {
        ## extract finaltime and finalsize
        finaltime <- simSum[2]
        finalsize <- simSum[6]
        
        ## create incidence counts from simulated counts
        ER <- c(NA, diff(counts[, 5]))
        EI <- c(NA, diff(counts[, 4])) + ER
        
        ## extract just day 55 onwards
        ER <- ER[55:length(ER)]
        EI <- EI[55:length(EI)]
        OR <- OR[56:length(OR)]
        OI <- OI[56:length(OI)]
        
        ER <- ER[OR > 0]
        OR <- OR[OR > 0]
        EI <- EI[OI > 0]
        OI <- OI[OI > 0]
        
        ## calculate chi-squared metrics
        X2I <- sum((EI - OI)^2 / OI)
        X2R <- sum((ER - OR)^2 / OR)
    }
    
    ## return vector if match, else return NA
    if(all(abs(c(finalsize, finaltime, X2I, X2R) - data) <= tols)){
        return(c(finalsize, finaltime, X2I, X2R))
    } else {
        return(NA)
    }
}

## now set tolerances
tols <- matrix(rep(round(seq(40, 5, length.out = 10)), each = 2), ncol = 2, byrow = T)
tols <- cbind(tols, matrix(rep(round(seq(600, 230, length.out = 10)), each = 2), ncol = 2, byrow = T))
colnames(tols) <- c("finalsize", "finaltime", "X2I", "X2R")

## define the targeted summary statistics
data <- c(finalsize = 316, finaltime = 191, X2I = 0, X2R = 0)
``

Now we run the ABC-SMC routine:

``{r, ebola-solabcsmc}
## run ABC-SMC algorithm
post <- ABCSMC(data, priors, simEbola, iniStates, 50, tols, 
               parallel = T, 
               model = model, tspan = 1:191, OI = ebola$clin_signs, OR = ebola$removal)
``

``{r, ebola-solR0}
## summarise approximate posterior
summary(post, transfunc = R0fn)
``

```

