# More challenging example: Ebola in the Democratic Republic of Congo

The rates of transition between states in this model is given by:

\begin{align*}
    P\left[\mbox{infection event in}~[t, t + \delta t)\right] = \beta S I / N + o(\delta t)\\
    P\left[\mbox{infectivity event in}~[t, t + \delta t)\right] = \delta E + o(\delta t)\\
    P\left[\mbox{removal event in}~[t, t + \delta t)\right] = \gamma I + o(\delta t)
\end{align*}

where $\beta$ is *time-dependent*, such that:

$$
    \beta = \left\{
    \begin{align}
        &\beta & \mathrm{if}~t < t_{int},\\
        &\beta e^{-q(t - t_{int})} & \mathrm{if}~t \geq t_{int},
    \end{align}\right.
$$

where $t_{int}$ is the time at which intervention strategies were introduced. The challenge here is that in a stochastic system we require that the event rates are piecewise constant, and thus the continuous exponential decay is hard to capture. Instead we can approximate this by assuming that the rate only changes after each event (in practice we would do this in a much better manner at the cost of additional computational loads).

To model this using our `mparseRcpp()` function, we need to add a little bit of additional C code to our model. In essence we need an **if-else** statement: **if** $t > t_{int}$, then multiply $\beta$ by $e^{-q(t - t_{int})}$, otherwise multiply it by 1. The C syntax for an if-else statement is of the form:

`(COND ? OUT1:OUT2)`

where `COND` is a condition, which if met outputs `OUT1`, else it outputs `OUT2`. Hence, to set up the model we can run:

```{r}
## set up model
transitions <- c(
    "S -> beta * exp((t > 126.0 ? (-q):0.0)) * S * I / (S + E + I + R) -> E", 
    "E -> delta * E -> I", 
    "I -> gamma * I -> R"
)
compartments <- c("S", "E", "I", "R")
pars <- c("beta", "delta", "gamma", "q")
model <- mparseRcpp(
    transitions = transitions, 
    compartments = compartments,
    pars = pars
)
```

```{r}
model <- compileRcpp(model)
model
```

```{task, title = "Question"}
Why is this an approximation? Is it an OK approximation do you think? Could you suggest a better approximation?
```

```{solution, title = "Answer"}
It's an approximation because we only update the rate every time there's an event. It might be an OK approximation, since if the rates are high, then we get lots of events and thus we update the infection rate more frequently. If rates are low, then we do not update the rates very frequently, but then the effect of the exponential decay is to set low rates after some period of time.

A better appraoch might be to update the infection rate at some regular interval, such as at daily time steps.
```

Now we can set priors and initial states:

```{r}
## set priors
priors <- data.frame(
    parnames = c("beta", "delta", "gamma", "q"), 
    dist = rep("gamma", 4), 
    stringsAsFactors = F
)
priors$p1 <- rep(2, 4)
priors$p2 <- c(10, 10, 1 / 0.07, 10)

## set initial states
iniStates <- c(S = 5364499, E = 0, I = 1, R = 0)
```

Now we set up the data to match to (final epidemic size and date of final removal):

```{r}
## define the targeted summary statistics
data <- c(finalsize = 316, finaltime = 191)
```

Finally we set a sequence of tolerances:

```{r}
## set tolerances
tols <- matrix(rep(round(seq(300, 40, length.out = 10)), each = 2), ncol = 2, byrow = T)
colnames(tols) <- c("finalsize", "finaltime")
```

We then set up a function to run the model, extract the final epidemic size and date of the final removal, and return the relevant measures:

```{r}
## function to match simulations
simEbola <- function(pars, data, tols, u, model) {
    ## run model
    sims <- model(pars, 0, data[2] + tols[2], u)
    
    ## this returns a vector of the form:
    ## completed (1/0), t, S, E, I, R (here)
    if(sims[1] == 0) {
        ## if simulation rejected
        return(NA)
    } else {
        ## extract finaltime and finalsize
        finaltime <- sims[2]
        finalsize <- sims[6]
    }
    
    ## return vector if match, else return NA
    if(all(abs(c(finalsize, finaltime) - data) <= tols)){
        return(c(finalsize, finaltime))
    } else {
        return(NA)
    }
}
```

```{r}
## set seed
set.seed(50)
```

Now we run the ABC-SMC algorithm (initially just for a single generation and for 10 particles---you'll see why in a moment):

```{r}
## run ABC-SMC algorithm
post <- ABCSMC(data, priors, simEbola, iniStates, 10, tols[1, ], parallel = T, model = model)
```

```{r, fig.width = 8, fig.height = 8}
## plot approximate posteriors
plot(post)
plot(post, "output")
```

```{task, title = "Question"}
We can see that the first generation of our algorithm took a long time to run. Why do you think this was?
```

```{solution, title = "Answer"}
This is for two main reasons. Firstly, the prior space is quite large relative to the posterior space, which means that a large number of parameters are generated that produce simulations that do not match the data. This means that lots of simulations have to be produce to get a fixed number of matches. Secondly, there is a large background population, which means that for some parameters we may simulate very large epidemics, which, although ultimately rejected, take a long time to evaluate.
```

## Speeding up simulations using stopping criteria

One way to speed up the algorithms is to note that if we have summary statistics that are **monotonically** increasing, then we can monitor these criteria during the simulation and reject the simulation as soon as the simulated summary measure is greater than the observed summary measure by more than the tolerance. This means that we do not have to run the simulation to completion to reject it.

We've already done this to a certain extent by only simulating up to time $T + \epsilon_T$, where $T$ is the final removal time. The rationale was the same; if the epidemic is still ongoing at time $T + \epsilon_T$, then by definition the difference between the simulated final removal time and the observed final removal time will be greater than the tolerance, and furthermore this distance will keep increasing. Hence we do not have to find out the exact final removal time in order to reject the simulation.

Here we will use a `stopCrit` argument to `mparseRcpp()` to define additional stopping criteria, based on the other summary statistics. The argument is defined as a **conditional statement** that defines when to reject. For example, to add a stopping criterion based on the final epidemic size, we can reject once $R^* > R_F + \epsilon_R$, where $R^*$ is the **simulated** number of removals and $R_F$ is the final epidemic size (the final **observed** number of removals). Hence,

```{r}
model <- mparseRcpp(
    transitions = transitions,
    compartments = compartments,
    pars = pars,
    addVars = c("finalsize", "tol_R"),
    stopCrit = "R > (finalsize + tol_R)"
)
```

Note that since the variables `finalsize` and `tol_R` do not exist in the model, we have to tell the parsing function to include them as additional arguments using the `addVars` argument. Once compiled, this adds to additional arguments to the `model()` function e.g.

```{r}
model <- compileRcpp(model)
model
```

It is now up to us to make sure that we pass `finalsize` and `tol_R` to the `model()` function. We can do this by amending the `simEbola()` function we wrote earlier:

```{r}
## function to match simulations
simEbola <- function(pars, data, tols, u, model) {
    ## run model
    sims <- model(pars, 0, data[2] + tols[2], u, data[1], tols[1])
    
    ## this returns a vector of the form:
    ## completed (1/0), t, S, E, I, R (here)
    if(sims[1] == 0) {
        ## if simulation rejected
        return(NA)
    } else {
        ## extract finaltime and finalsize
        finaltime <- sims[2]
        finalsize <- sims[6]
    }
    
    ## return vector if match, else return NA
    if(all(abs(c(finalsize, finaltime) - data) <= tols)){
        return(c(finalsize, finaltime))
    } else {
        return(NA)
    }
}
```

Now we run again:

```{r}
## run ABC-SMC algorithm
post <- ABCSMC(data, priors, simEbola, iniStates, 10, tols[1, ], parallel = T, model = model)
```

Now we can see that the first generation in particular runs much faster than before since some simulations can be rejected sooner without loss of accuracy. Now we will increase the number of particles and run properly.

```{r}
## run ABC-SMC algorithm
post <- ABCSMC(data, priors, simEbola, iniStates, 50, tols, parallel = T, model = model)
```

Once we have a set of (approximate) posterior samples, we can create posteriors for derived variables of interest, such as $R_0$, the mean infectious period, and the mean incubation period:

```{r}
## function to calculate R0 and length of
## epidemiological periods
R0fn <- function(beta, delta, gamma) {
    data.frame(R0 = beta / gamma, latperiod = 1 / delta, infperiod = 1 / gamma)
}

## summarise approximate posterior
summary(post, transfunc = R0fn)
```

Since this ran relatively quickly, we'll now run for another few generations...

```{r}
tols <- matrix(rep(seq(30, 10, by = -10), each = 2), ncol = 2, byrow = T)
colnames(tols) <- c("finalsize", "finaltime")

## run ABC-SMC algorithm
post <- ABCSMC(post, tols, parallel = T)
```

Now summarise the outputs:

```{r, fig.width = 10, fig.height = 5}
## summarise approximate posterior
summary(post, transfunc = R0fn)
```

```{r, fig.width = 8, fig.height = 8}
## plot approximate posteriors
plot(post, transfunc = R0fn, gen = c(1, 5, 13))
```

```{r}
## plot accepted outputs
plot(post, "output", gen = c(1, 5, 13))
```
